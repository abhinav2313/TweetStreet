{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests as req\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from config import (consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "from datetime import datetime, date, time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#Setup Tweepy API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "#Saving real time stock data in  file\n",
    "stockHistory = 'stockHistory.txt'\n",
    "# Base URL for GET requests to retrieve stock data\n",
    "url = \"https://api.iextrading.com/1.0/stock/\"\n",
    "#Opening file\n",
    "file=open(stockHistory,'w')\n",
    "\n",
    "#Stock ticker for collecting data\n",
    "stocks = ['AAPL', 'V', 'AMZN', 'WMT', 'NFLX', 'NKE', 'M']\n",
    "# Specify the companies\n",
    "target_terms = [\"@Apple\", \"@Visa\", \"@amazon\", \"@Walmart\", \"@netflix\", \"@Nike\", \"@Macys\"]\n",
    "\n",
    "# Loop through (can change the range if we need to)\n",
    "def getSentimentAnalysis(handler):\n",
    "    # Loop through all companies\n",
    "    \n",
    "   # Make a dictionary to hold each type of sentiment\n",
    "        company_sentiment = {\n",
    "            \"compound_list\": [], \n",
    "            \"positive_list\": [], \n",
    "            \"negative_list\": [], \n",
    "            \"neutral_list\": []\n",
    "        }\n",
    "        public_tweets = api.search(handler, count=100, result_type=\"recent\")\n",
    "\n",
    "        # Loop through all tweets\n",
    "        for tweet in public_tweets[\"statuses\"]:\n",
    "\n",
    "            # Run Vader Analysis on each tweet\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            compound = results[\"compound\"]\n",
    "            pos = results[\"pos\"]\n",
    "            neu = results[\"neu\"]\n",
    "            neg = results[\"neg\"]\n",
    "\n",
    "            # Save all info to the corresponding lists\n",
    "            company_sentiment[\"compound_list\"].append(compound)\n",
    "            company_sentiment[\"positive_list\"].append(pos)\n",
    "            company_sentiment[\"neutral_list\"].append(neu)\n",
    "            company_sentiment[\"negative_list\"].append(neg)\n",
    "        list_of_sentiment = [np.mean(company_sentiment[\"compound_list\"]),np.mean(company_sentiment[\"positive_list\"]),\n",
    "                np.mean(company_sentiment[\"neutral_list\"]),np.mean(company_sentiment[\"negative_list\"])]\n",
    "        return list_of_sentiment\n",
    "print(str(datetime.now().time()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stock_market_open_time = time(hour=8, minute=30, second=0, microsecond=0)\n",
    "stock_market_close_time = time(hour=23, minute=0, second=0, microsecond=0)\n",
    "\n",
    "file.write(\" Counter  ,\"+\"Corporation          ,\"+\"Time       ,\"+\"Stock Ticker       ,\"+\"Stock Price       ,\"+\n",
    "           \"Compound         ,\"+\"Positive         ,\"+\"Neutral        ,\"+\"Negative        \\n\")\n",
    "def stockPriceCollectionWithSentiment():\n",
    "    counter = 0;\n",
    "    while((datetime.now().time() > stock_market_open_time) & (datetime.now().time() < stock_market_close_time) & counter < 50):\n",
    "        counter = counter+1\n",
    "        time_var = datetime.now()\n",
    "        for stock in stocks:\n",
    "            print(url+stock+\"/quote\")\n",
    "            response = req.get(url+stock+\"/quote\")\n",
    "            print(\"Counter::\"+str(counter))\n",
    "            print(response.json())\n",
    "            sentiment_anal = getSentimentAnalysis(target_terms[stocks.index(stock)])\n",
    "            print(\"Compound::\",sentiment_anal[0])\n",
    "            print(\"Positive::\",sentiment_anal[1])\n",
    "            print(\"Neutral::\",sentiment_anal[2])\n",
    "            print(\"Negative::\",sentiment_anal[3])\n",
    "            print(f\"{counter} ,{response.json()['companyName']}          ,{time_var}        ,{response.json()['symbol']}     ,{response.json()['latestPrice']}     ,\" \n",
    "                       +f\"{sentiment_anal[0]}      ,{sentiment_anal[1]}      ,{sentiment_anal[2]}      ,{sentiment_anal[3]}\\n\")\n",
    "            file.write(f\"{counter} ,{response.json()['companyName']}          ,{time_var}        ,{response.json()['symbol']}     ,{response.json()['latestPrice']}     ,\" \n",
    "                       +f\"{sentiment_anal[0]}      ,{sentiment_anal[1]}      ,{sentiment_anal[2]}      ,{sentiment_anal[3]}\\n\")\n",
    "        file.flush()\n",
    "            # typically the above line would do. however this is used to ensure that the file is written\n",
    "        os.fsync(file.fileno())\n",
    "        t.sleep(30)\n",
    "try:\n",
    "    stockPriceCollectionWithSentiment()\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n\\nKeyboard exception received. Exiting.')\n",
    "    file.close()\n",
    "    exit()\n",
    "file.close();\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
